{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'><img src='../Pierian_Data_Logo.png'/></a>\n",
    "___\n",
    "<center><em>Авторские права принадлежат Pierian Data Inc.</em></center>\n",
    "<center><em>Для дополнительной информации посетите наш сайт <a href='http://www.pieriandata.com'>www.pieriandata.com</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Извлечение признаков из текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот блокнот разделён на две части:\n",
    "* Сначала мы выясним, что нам понадобится для построения инструментария NLP (Natural Language Processing), который превратит набор текста в числовой массив из *признаков*. Для этого мы вручную вычислим, как часто встречаются те или иные слова, и построим метрику TF-IDF.\n",
    "* Далее мы выполним эти шаги с помощью scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1: основные принципы извлечения признаков из текста\n",
    "\n",
    "\n",
    "В этой части мы с помощью базовых операций Python построим очень простую систему NLP. Мы возьмём *набор документов* (*corpus of documents*) - это будет два текстовых файла. Далее создадим *словарь* (*vocabulary*) из всех слов обоих документов. И затем посмотрим на технику *Мешок слов* (*Bag of Words*) для извлечения признаков из каждого документа.<br>\n",
    "<div class=\"alert alert-info\" style=\"margin: 20px\">В этом разделе мы только приводим иллюстрации основных принципов!\n",
    "<br>Не обращайте внимание на то, как пишется код в Python - позднее мы применим для этой задачи Scikit-Learn.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начнём с документов:\n",
    "Для простоты изложения в наших текстовых файлах One.txt и Two.txt не будет каких-либо знаков пунктуации. Давайте откроем эти файлы и прочитаем данные. Обратите внимание, что если в будущем у Вас файлы будут большие, то их не следует выводить на экран полностью.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаём словарь vocabulary (Мешок слов - \"Bag of Words\")\n",
    "\n",
    "Для этого мы возьмём все слова из обоих документов, найдём уникальные слова, и пронумеруем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about', 'canine', 'furry', 'animals', 'a', 'dogs', 'are', 'is', 'pets', 'story', 'this', 'our'}\n"
     ]
    }
   ],
   "source": [
    "# Создаем мешок уникальных слов из файла 1\n",
    "with open('One.txt') as mytext:\n",
    "    bag_a = mytext.read().lower().split()\n",
    "    unique_bag_a = set(bag_a)\n",
    "    print(unique_bag_a)\n",
    "    # a = mytext.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about', 'water', 'popular', 'catching', 'waves', 'a', 'sport', 'fun', 'surfing', 'is', 'story', 'this'}\n"
     ]
    }
   ],
   "source": [
    "# Создаем мешок уникальных слов из файла 2\n",
    "with open('Two.txt') as mytext:\n",
    "    bag_b = mytext.read().lower().split()\n",
    "    unique_bag_b = set(bag_b)\n",
    "    print(unique_bag_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'canine', 'animals', 'popular', 'catching', 'waves', 'dogs', 'is', 'pets', 'story', 'this', 'our', 'about', 'water', 'furry', 'a', 'are', 'fun', 'surfing', 'sport'}\n"
     ]
    }
   ],
   "source": [
    "# Объединяем\n",
    "all_unique_wods = set()\n",
    "all_unique_wods.update(unique_bag_a)\n",
    "all_unique_wods.update(unique_bag_b)\n",
    "print(all_unique_wods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'canine': 0, 'animals': 1, 'popular': 2, 'catching': 3, 'waves': 4, 'dogs': 5, 'is': 6, 'pets': 7, 'story': 8, 'this': 9, 'our': 10, 'about': 11, 'water': 12, 'furry': 13, 'a': 14, 'are': 15, 'fun': 16, 'surfing': 17, 'sport': 18}\n"
     ]
    }
   ],
   "source": [
    "# Нам надо пронумеровать слова. Для этого создаем словар\n",
    "full_vocab = dict()\n",
    "# print(type(full_vocab))    \n",
    "i = 0\n",
    "for word in all_unique_wods:\n",
    "    full_vocab[word] = i\n",
    "    i = i + 1\n",
    "print(full_vocab)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мешок слов - считаем, как часто встречаются отдельные слова\n",
    "\n",
    "После того, как мы создали словарь из слов, давайте выполним *извлечение признаков* для каждого из двух исходных документов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создаём пустые счётчики для каждого документа**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Добавляем счётчики слов для каждого документа:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Для каждого документа создаём пустые счётчики для всех слов из словаря full_vocab:\n",
    "one_freq = [0]*len(full_vocab)\n",
    "# print(type(one_freq))\n",
    "two_freq = [0]*len(full_vocab)\n",
    "# Берём слова из файла, для каждого слова ищем соответствие в нашем словаре full_vocab:\n",
    "with open('One.txt') as f:\n",
    "    one_text = f.read().lower().split()\n",
    "\n",
    "for word in one_text:\n",
    "    word_ind = full_vocab[word]\n",
    "    one_freq[word_ind]+=1\n",
    "print(one_freq)\n",
    "# В качестве индекса в full_vocab[word] у нас конкретное слово,\n",
    "#  а в full_vocab[word] - цифра, соответствующая этому слову.\n",
    "#  В one_freq[word_ind] мы переаем эту цифру в качестве \n",
    "# индекса (что по существу является позицией в списке (list))\n",
    "# и на этой позиции увеличиваем знаачение на 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 1, 0, 3, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# То же самое для второго документа:\n",
    "with open('Two.txt') as f:\n",
    "    two_text = f.read().lower().split()\n",
    "    \n",
    "for word in two_text:\n",
    "    word_ind = full_vocab[word]\n",
    "    two_freq[word_ind]+=1\n",
    "print(two_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'list'>\n",
      "['canine', 'animals', 'popular', 'catching', 'waves', 'dogs', 'is', 'pets', 'story', 'this', 'our', 'about', 'water', 'furry', 'a', 'are', 'fun', 'surfing', 'sport']\n"
     ]
    }
   ],
   "source": [
    "# В отличии от full_vocab, который является словарем, где данные \n",
    "# не упорядочены, dв списке они упорядочены.\n",
    "all_words = ['']*len(full_vocab)\n",
    "# print(type(full_vocab))\n",
    "# print(type(all_words))\n",
    "\n",
    "for word in full_vocab:\n",
    "    word_ind = full_vocab[word]\n",
    "    all_words[word_ind] = word\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   canine  animals  popular  catching  waves  dogs  is  pets  story  this  \\\n",
      "0       1        1        0         0      0     2   1     1      1     1   \n",
      "1       0        0        1         1      1     0   3     0      1     1   \n",
      "\n",
      "   our  about  water  furry  a  are  fun  surfing  sport  \n",
      "0    1      1      0      1  1    1    0        0      0  \n",
      "1    0      1      1      0  1    0    1        2      1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bag_of_word = pd.DataFrame(data=[one_freq,two_freq],columns=all_words)\n",
    "print(bag_of_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые слова есть в обоих документах, а некоторые слова есть только в `One.txt`, и некоторые другие слова есть только в `Two.txt`. Если мы применим эту логику для десятков тысяч документов, то наш словарь вполне сможет вырасти до десятков тысяч слов. При этом матрицы будут содержать очень много нулей - это будут **разреженные метрицы** (**sparse matrices**).\n",
    "\n",
    "\n",
    "# Продолжение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мешок слов и Tf-idf\n",
    "В приведённом выше примере, каждый вектор можно рассматривать как мешок слов (*bag of words*). Сами по себе эти вектора не очень полезны, но мы также можем добавить к ним частоту слов (*term frequencies - TF*) - как часто то или иное слово встречается в документе. Простой способ сделать это - это посчитать, сколько раз встречается слово в документе, и разделить на общее количество слов в документе. Тогда можно сравнивать между большими и маленькими документами, сколько раз то или иное слово встречается в документе.\n",
    "\n",
    "Однако, если какое-то слово встречается во многих документах, то такое слово не будет являться хорошим признаком для отделения документов друг от друга. Чтобы работать с такими словами, можно добавить метрику *inverse document frequency (IDF)*, которая вычисляется как общее количество документов, разделить на количество документов, в которых содержится рассматриваемое нами слово (слово, для которого вычисляется IDF). В практических задачах это значение приводится к логарифмической шкале, подробнее см. [здесь](https://ru.wikipedia.org/wiki/TF-IDF).\n",
    "\n",
    "Соединяя метрики TF (Term Frequency) и IDF (Inverse Document Frequency), мы получаем метрику [**tf-idf**](https://ru.wikipedia.org/wiki/TF-IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова и морфология слов\n",
    "Некоторые слова встречаются слишком часто, например слова \"the\" и \"and\" в английском языке. Эти слова можно просто исключить из рассмотрения. \n",
    "\n",
    "Кроме этого, одно и то же слово может встречаться в единственном и множественном числе, а также в различных падежах. Нам бы хотелось записать это слово один раз, а не записывать различные его вариации (например, записать только `cat` для обоих слов `cat` и `cats`). Это позволит нам уменьшить размер словаря и увеличить скорость работы модели.\n",
    "\n",
    "С другой стороны, различные падежи слов - особенно в русском языке - несут дополнительную смысловую информацию. Эту информацию по-хорошему можно сохранять, то есть ссылаться на базовое слово в словаре, плюс сохранять дополнительные атрибуты этого слова в конкретном месте текста. В нашем блокноте мы ограничимся лишь базовым подсчётом частоты слов - как в отдельном документе, так и в целом наборе документов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2: извлечение признаков и текста в Scikit-Learn\n",
    "\n",
    "Давайте применим sklearn для решения этих задач!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Варианты извлечения признаков в Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['This is a line',\n",
    "           \"This is another line\",\n",
    "       \"Completely different line\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer - подсчитывает сколько раз определенное слова встречается в документе\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# здесь каждая строка ообрабатывается как отдельный документ\n",
    "# fit - формирует словарь, transform - подсчитывает слова\n",
    "sparse_mat = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 1, 1, 1],\n",
       "        [1, 0, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_mat.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 5, 'is': 3, 'line': 4, 'another': 0, 'completely': 1, 'different': 2}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About stop-word https://gist.github.com/sebleier/554280\n",
    "cv = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(text).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line': 2, 'completely': 0, 'different': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfTransformer\n",
    "\n",
    "TfidfVectorizer применяется для текстовых документов, а TfidfTransformer применяется к матрице со счётчиками, которую возвращает CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIdfVectorizer\n",
    "Выполняем оба действия единым шагом!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer - объединяет действия CountVectorizer и TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_res = tv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_res.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________\n",
    "Ниже не понятно, что это ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cv',CountVectorizer()),('tfidf',TfidfTransformer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipe.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.61980538, 0.48133417,\n",
       "         0.61980538],\n",
       "        [0.63174505, 0.        , 0.        , 0.4804584 , 0.37311881,\n",
       "         0.4804584 ],\n",
       "        [0.        , 0.65249088, 0.65249088, 0.        , 0.38537163,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.todense()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
